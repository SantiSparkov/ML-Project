{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spark/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spark/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 - 8s - 30ms/step - accuracy: 0.1296 - loss: 2.6802 - val_accuracy: 0.1682 - val_loss: 2.4430\n",
      "Epoch 2/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.1449 - loss: 2.4960 - val_accuracy: 0.1682 - val_loss: 2.4267\n",
      "Epoch 3/20\n",
      "276/276 - 8s - 29ms/step - accuracy: 0.1499 - loss: 2.4541 - val_accuracy: 0.1664 - val_loss: 2.4145\n",
      "Epoch 4/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.1646 - loss: 2.4063 - val_accuracy: 0.1691 - val_loss: 2.4242\n",
      "Epoch 5/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.1775 - loss: 2.3648 - val_accuracy: 0.1686 - val_loss: 2.4398\n",
      "Epoch 6/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.1874 - loss: 2.3387 - val_accuracy: 0.1641 - val_loss: 2.4754\n",
      "Epoch 7/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.1942 - loss: 2.3153 - val_accuracy: 0.1623 - val_loss: 2.4708\n",
      "Epoch 8/20\n",
      "276/276 - 8s - 29ms/step - accuracy: 0.1987 - loss: 2.2917 - val_accuracy: 0.1578 - val_loss: 2.5138\n",
      "Epoch 9/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.2032 - loss: 2.2884 - val_accuracy: 0.1636 - val_loss: 2.5167\n",
      "Epoch 10/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.2087 - loss: 2.2660 - val_accuracy: 0.1646 - val_loss: 2.5651\n",
      "Epoch 11/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.2145 - loss: 2.2536 - val_accuracy: 0.1600 - val_loss: 2.6118\n",
      "Epoch 12/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.2194 - loss: 2.2406 - val_accuracy: 0.1632 - val_loss: 2.6419\n",
      "Epoch 13/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.2222 - loss: 2.2318 - val_accuracy: 0.1636 - val_loss: 2.6848\n",
      "Epoch 14/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.2239 - loss: 2.2271 - val_accuracy: 0.1623 - val_loss: 2.7887\n",
      "Epoch 15/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.2250 - loss: 2.2266 - val_accuracy: 0.1646 - val_loss: 2.7341\n",
      "Epoch 16/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.2255 - loss: 2.2167 - val_accuracy: 0.1636 - val_loss: 2.7708\n",
      "Epoch 17/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.2264 - loss: 2.2148 - val_accuracy: 0.1609 - val_loss: 2.7859\n",
      "Epoch 18/20\n",
      "276/276 - 8s - 29ms/step - accuracy: 0.2255 - loss: 2.2153 - val_accuracy: 0.1623 - val_loss: 2.7469\n",
      "Epoch 19/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.2274 - loss: 2.2125 - val_accuracy: 0.1609 - val_loss: 2.8034\n",
      "Epoch 20/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.2325 - loss: 2.2000 - val_accuracy: 0.1600 - val_loss: 2.8942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 - 0s - 7ms/step - accuracy: 0.1600 - loss: 2.8942\n",
      "Validation Accuracy: 16.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import json\n",
    "from joblib import dump\n",
    "\n",
    "# Cargar el dataframe\n",
    "data = pd.read_csv('./dataset/train.csv')\n",
    "\n",
    "# 1. Preprocesamiento de texto\n",
    "def convert_to_numeric(value):\n",
    "    if isinstance(value, str):\n",
    "        if 'K' in value:\n",
    "            return float(value.replace('K', '')) * 1_000\n",
    "        elif 'M' in value:\n",
    "            return float(value.replace('M', '')) * 1_000_000\n",
    "    return float(value)\n",
    "\n",
    "data[\"Summary\"] = data[\"Summary\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Crear y ajustar el tokenizer para la columna \"Summary\"\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data[\"Summary\"])\n",
    "sequences = tokenizer.texts_to_sequences(data[\"Summary\"])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Guardar el tokenizer como JSON\n",
    "with open(\"tokenizer.json\", \"w\") as f:\n",
    "    json.dump(tokenizer.to_json(), f)\n",
    "\n",
    "# Longitud máxima de secuencia\n",
    "max_length = 50\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Codificar las etiquetas (género)\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"Genre\"] = label_encoder.fit_transform(data[\"Genre\"])\n",
    "labels = to_categorical(data[\"Genre\"])\n",
    "\n",
    "# Dividir datos en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Definir el modelo\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(labels.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# 3. Compilar el modelo\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 4. Entrenar el modelo\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 5. Guardar el modelo con joblib\n",
    "model.save(\"game_genre_model.h5\")  # Guardar el modelo en formato HDF5 para TensorFlow\n",
    "\n",
    "# 6. Evaluar el modelo\n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=2)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predictions saved to './dataset/predicted_random_sample_genres.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "# Load the training data again\n",
    "train_data = pd.read_csv('./dataset/train.csv')\n",
    "\n",
    "# Randomly select a subset of rows from the training data for prediction (e.g., 100 samples)\n",
    "random_sample = train_data.sample(n=100, random_state=42)\n",
    "\n",
    "# Preprocess the 'Summary' column in the random sample\n",
    "random_sample[\"Summary\"] = random_sample[\"Summary\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Convert the random sample 'Summary' to sequences and pad them\n",
    "sample_sequences = tokenizer.texts_to_sequences(random_sample[\"Summary\"])\n",
    "sample_padded_sequences = pad_sequences(sample_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(sample_padded_sequences, verbose=1)\n",
    "\n",
    "# Convert predictions to genre labels\n",
    "predicted_genres = label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
    "\n",
    "# Save results to a new CSV file\n",
    "output = pd.DataFrame({\n",
    "    'id': random_sample['id'], \n",
    "    'Genre': predicted_genres\n",
    "})\n",
    "output.to_csv('./dataset/predicted_random_sample_genres.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to './dataset/predicted_random_sample_genres.csv'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
